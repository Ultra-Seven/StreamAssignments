%!TEX root = ../main.tex


\section{Experiments}
\label{sec:experiments}

We run a series of micro and end-to-end system experiments to understand the performance improvements that each component contributes, as well as a user study to measure user-perceived latency.  These experiments use three different interactive visualization front-end designs that vary in the number of possible interactions that the user can perform---a simple pan-and-zoom map interface~\cite{}, a complex cross-filtering application, and an interface that is button-heavy.

\begin{figure*}
  \centering
   \begin{subfigure}[t]{.32\textwidth}
    \includegraphics[width = .99\columnwidth]{figures/nofig}
    \caption{Simple map exploration visualization.}
    \label{f:vis_map} 
  \end{subfigure}
  \begin{subfigure}[t]{.32\textwidth}
    \includegraphics[width = .99\columnwidth]{figures/nofig}
    \caption{Medium cross-filtering interface.}
    \label{f:vis_xfilter} 
  \end{subfigure}
  \begin{subfigure}[t]{.32\textwidth}
    \includegraphics[width = .99\columnwidth]{figures/nofig}
    \caption{Complex Tableau-like interface.}
    \label{f:vis_tableau} 
  \end{subfigure}
  \caption{Interactive visualization designs that vary in interaction complexity.  
  Simple map visualization with 9 possible button-based interactions (a), cross filter visualization with two continuous sliders (b),
Tableau-like interface with XXX possible interactions (c).}

  \label{f:interfaces}
\end{figure*}



\subsection{Experimental Setup}
This subsection describes our experimental setup and metrics.

\stitle{Interactive Visualizations: }
Figure~\ref{f:interfaces} shows the three interactive visualization interfaces used in the experiments.  The interfaces vary in interaction complexity (e.g., number of possible user interactions).  Figure~\ref{f:vis_map} is a button-based map interface that supports panning and zooming;  Figure~\ref{f:vis_xfilter} supports continuous selection interactions, where drawing a selection in a given view dynamically updates the data in nthe other views; Figure~\ref{f:vis_tableau} is a full featured visual data exploration interface that supports continuous interactions such as cross-filtering and sliders, as well as discrete button-based interactions. 

\stitle{Traces: }
In our system experiments, we use \ewu{XXX} user traces collected from a Chromium extension running on the authors' browsers.  These traces were collected over the span of \ewu{XXX} weeks for all webpages that the authors browser.  The trace tracks mouse events (e.g., click, move, drag), as well as the type of page element that the user interacted with. 

We also collected a specialized set of user traces when interacting with custom interactive visualizations used in the user study, and labeled the type of interaction (e.g., button click, slider drag, pan, zoom in, etc).  For the visualization-based traces, we also logged the corresponding query requests for each mouse event in order to collect the ground truth.  The traces, visualizations, and queries will be released after publication.

To summarize, our traces conform to the following schemas:
{\small \begin{verbatim}
 events(eid, user, time, x, y, action, url, label)
queries(qid, eid, querystring)
\end{verbatim}}

\stitle{Conditions: } 
We use a request-response baseline ($Base_{acc}^{c}$) that performs query pre-fetching $400$ms into the future using a query prediction model with accuracy $acc$ and a FIFO cache size that can store $c$ query results.   This allows our baseline to reproduce prediction and caching characteristics from prior prefetching-based papers in a general manner.  We evaluate \sys by varying the accuracy of the mouse prediction model, the scheduling parameters, and the type of progressive encoding.  

\stitle{Metrics: }
In addition to reporting standard mouse prediction accuracy on the user traces, we report metrics for visualization quality and performance:

\begin{itemize}[leftmargin=*, topsep=0mm, itemsep=0mm]
  \item {\it Visualization Metrics: } Since \sys quickly renders visualizations that progressively improve over time, we introduce two ways to measure how accurate the visualization is.  {\it Value Error ($\epsilon_v$)} compares the difference between each mark's pixel coordinates in the progressive visualization against their coordinates in the final visualization.  For instance, if the results are rendered as a scatterplot, then we compare each point's x and y coordinates; if rendered as a bar chart, we compare each bar's pixel height.  {\it Pixel Error ($\epsilon_p$)} follows the procedure in M4~\cite{m4} and measures the number of pixels that differ in value between the progressive and final visualization.  For each measure, we report the median and $\pm 1\sigma$ bounds across time.  In addition, during our user study, we report the percentage of user interactions that achieve different $\epsilon_v$ bounds.

  \item {\it Performance Metrics: }  We report latency from user interaction to first visualization ($l_{1st}$), as well as latency until $\epsilon_v$ is below XXX ($l_{\epsilon_v}$).  
\end{itemize}

\subsection{Microbenchmarks}

This set of experiments highlight the characteristics of the mouse prediction and network schedulers in isolation.

\subsubsection{Mouse Prediction Models}

\subsubsection{Network Scheduling}

Vary: network throughput and latency (via client/server-side artificial delays)

\subsubsection{Progressive Encoding}

\subsection{Macrobenchmarks}
In these experiments, we replay $N$ random user traces on each interactive visualization 

\subsection{User-Study}



